{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Playing Pong using the DQN algorithm\n",
    "\"\"\"\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network for appoximation of the Q function\n",
    "    Model Parameters like in Mnih et al., 2015\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions=4, feature_size=3136):\n",
    "        super(DQN, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self.feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, self.n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Object for saving the memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        \"\"\"\n",
    "        Saves an experience or just one timepoint\n",
    "        :param experience:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.position >= self.capacity:\n",
    "            self.position = 0\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = experience\n",
    "        self.position += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Take a random batch from the memory\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Experience:\n",
    "    \"\"\"\n",
    "    Class with slots to save an experience\n",
    "    \"\"\"\n",
    "    __slots__ = ['state', 'action', 'reward', 'next_state', 'non_final']\n",
    "\n",
    "    def __init__(self, state, action, reward, next_state):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.non_final = next_state is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon, q_network, n_actions):\n",
    "    \"\"\"\n",
    "    Choose an action given a state, epsilon and the q_network\n",
    "    :param state: \n",
    "    :param epsilon: \n",
    "    :param q_network: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor(random.randrange(n_actions))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(q_network.forward(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_frame(frame):\n",
    "    \"\"\"\n",
    "    Transform a frame to tensor of shape (batch_size, 1, 84, 84)\n",
    "    :param frame:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    frame = T.Compose([T.ToPILImage(), T.Resize((84, 84)), T.Grayscale(), T.ToTensor()])(frame)\n",
    "    frame = frame / 255\n",
    "    return frame\n",
    "\n",
    "def game_step(env, action, n_steps=4):\n",
    "    \"\"\"\n",
    "    Play one step of the game\n",
    "    :param env:\n",
    "    :param action:\n",
    "    :n_steps: number of frames to play\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    reward = 0\n",
    "    last_state = False\n",
    "    for i in range(n_steps):\n",
    "        frame, r, done, _ = env.step(action)\n",
    "        if done:\n",
    "            last_state = True\n",
    "            reward += r\n",
    "        else:\n",
    "            frames.append(transform_frame(frame))\n",
    "            reward += r\n",
    "    if last_state:\n",
    "        state = None\n",
    "    else:\n",
    "        state = torch.stack(frames, 0)\n",
    "        state = state.transpose(0, 1)\n",
    "\n",
    "    return (state, reward, last_state)\n",
    "\n",
    "def extract_batch(batch, slot):\n",
    "    if slot == 'action':\n",
    "        return torch.tensor([x.action for x in batch])\n",
    "    elif slot == 'state':\n",
    "        return torch.cat([x.state for x in batch])\n",
    "    elif slot == 'reward':\n",
    "        return torch.tensor([x.reward for x in batch])\n",
    "    elif slot == 'next_state':\n",
    "        # return only non-final next states\n",
    "        next_states = [x.next_state for x in batch]\n",
    "        non_final_next_states = torch.cat([s for s in next_states if s is not None])\n",
    "        return non_final_next_states\n",
    "    elif slot == 'non_final':\n",
    "        return torch.tensor([x.non_final for x in batch])\n",
    "    else:\n",
    "        raise Exception(\"Incorrect Experience slot specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(policy, target, memory, optimizer, criterion, batch_size=32, gamma=0.99, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Calculate loss for one batch and perform optimization\n",
    "    :param policy:\n",
    "    :param memory:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # check if enough memory has been aquired\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample batch from memory\n",
    "    batch = memory.sample(batch_size)\n",
    "\n",
    "    # extract state, action, reward, next_state\n",
    "    action_batch = extract_batch(batch, 'action').to(device).unsqueeze(1)\n",
    "    state_batch = extract_batch(batch, 'state').to(device)\n",
    "    reward_batch = extract_batch(batch, 'reward').to(device)\n",
    "    next_state_batch = extract_batch(batch, 'next_state').to(device)\n",
    "    non_final_mask = extract_batch(batch, 'non_final')\n",
    "    \n",
    "    # q-values\n",
    "    q_value = policy(state_batch)\n",
    "    q_value = q_value.gather(1, action_batch).squeeze(1)\n",
    "\n",
    "    # q-values for next state\n",
    "    target_q_value_all = torch.zeros(batch_size, device=device)\n",
    "    target_q_value = target(next_state_batch)\n",
    "    _, max_idx = torch.max(target_q_value, dim=1)\n",
    "    target_q_value = target_q_value.gather(1, max_idx.unsqueeze(1)).squeeze(1)\n",
    "    target_q_value_all[non_final_mask] = target_q_value\n",
    "    \n",
    "    # expected q-value\n",
    "    expected_q_value = reward_batch + (gamma * target_q_value_all)\n",
    "\n",
    "    loss = criterion(q_value, expected_q_value)\n",
    "\n",
    "    # Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -21.0, Epsilon: 0.9896710000000157, Memory: 313, Frames: 313\n",
      "Reward: -20.0, Epsilon: 0.9785500000000327, Memory: 650, Frames: 650\n",
      "Reward: -21.0, Epsilon: 0.9686830000000477, Memory: 949, Frames: 949\n",
      "Reward: -21.0, Epsilon: 0.9582550000000636, Memory: 1265, Frames: 1265\n",
      "Reward: -21.0, Epsilon: 0.9485530000000784, Memory: 1559, Frames: 1559\n",
      "Reward: -21.0, Epsilon: 0.939577000000092, Memory: 1831, Frames: 1831\n",
      "Reward: -21.0, Epsilon: 0.9299740000001067, Memory: 2122, Frames: 2122\n",
      "Reward: -19.0, Epsilon: 0.918589000000124, Memory: 2467, Frames: 2467\n",
      "Reward: -21.0, Epsilon: 0.9095470000001378, Memory: 2741, Frames: 2741\n",
      "Reward: -20.0, Epsilon: 0.900175000000152, Memory: 3025, Frames: 3025\n",
      "Reward: -20.0, Epsilon: 0.8893180000001686, Memory: 3354, Frames: 3354\n",
      "Reward: -20.0, Epsilon: 0.8789560000001844, Memory: 3668, Frames: 3668\n",
      "Reward: -21.0, Epsilon: 0.870673000000197, Memory: 3919, Frames: 3919\n",
      "Reward: -21.0, Epsilon: 0.8613670000002112, Memory: 4201, Frames: 4201\n",
      "Reward: -21.0, Epsilon: 0.8527210000002243, Memory: 4463, Frames: 4463\n",
      "Reward: -21.0, Epsilon: 0.844372000000237, Memory: 4716, Frames: 4716\n",
      "Reward: -21.0, Epsilon: 0.8360560000002497, Memory: 4968, Frames: 4968\n",
      "Reward: -21.0, Epsilon: 0.8267830000002638, Memory: 5249, Frames: 5249\n"
     ]
    }
   ],
   "source": [
    "#=== PARAMETERS ===#\n",
    "batch_size = 32\n",
    "max_screens = 1400000\n",
    "memory_capacity = 80000\n",
    "memory_init_size = 500\n",
    "gamma = 0.99\n",
    "target_update = 10000\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.01\n",
    "epsilon_steps = 30000\n",
    "n_steps = 4\n",
    "lr = 0.0000625\n",
    "\n",
    "# Create Breakout environment\n",
    "env = gym.make('Pong-v0')\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# CNNs\n",
    "policy = DQN(n_actions=n_actions).cuda()\n",
    "target = DQN(n_actions=n_actions).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "\n",
    "# Loss\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "# Memory\n",
    "memory = ReplayBuffer(memory_capacity)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "epsilon_delta = (epsilon_start - epsilon_end)/epsilon_steps\n",
    "\n",
    "screens = 0\n",
    "\n",
    "while screens < max_screens:\n",
    "    \n",
    "    # reset env for new episode\n",
    "    env.reset()\n",
    "    # get initial state\n",
    "    state, _, _ = game_step(env, env.action_space.sample(), n_steps=n_steps)\n",
    "    complete_reward = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    for t in count():\n",
    "        screens += 1\n",
    "        \n",
    "        # adjust epsilon\n",
    "        epsilon = epsilon - epsilon_delta\n",
    "        if epsilon <= epsilon_end:\n",
    "            epsilon = epsilon_end\n",
    "        \n",
    "        # choose action based on current state\n",
    "        action = choose_action(state.cuda(), epsilon, policy, n_actions)\n",
    "        \n",
    "        # make one step\n",
    "        next_state, reward, done = game_step(env, action, n_steps=n_steps)\n",
    "        complete_reward += reward\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "    \n",
    "        # save the current experience\n",
    "        memory.push(Experience(state, action, reward, next_state))\n",
    "        \n",
    "        # update state variable\n",
    "        state = next_state\n",
    "        \n",
    "        if screens > memory_init_size:\n",
    "            # Perform one step of training on the policy network\n",
    "            training_step(policy, target, memory, optimizer, criterion=criterion, batch_size=batch_size, gamma=gamma)\n",
    "\n",
    "            # Update the target network after 10000 frames seen\n",
    "            if screens % target_update == 0:\n",
    "                target.load_state_dict(policy.state_dict())\n",
    "                print(\"update target\")\n",
    "\n",
    "        if done:\n",
    "            mem_len = len(memory.memory)\n",
    "            print(f\"Reward: {complete_reward}, Epsilon: {epsilon}, Memory: {mem_len}, Frames: {screens}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-env] *",
   "language": "python",
   "name": "conda-env-rl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
