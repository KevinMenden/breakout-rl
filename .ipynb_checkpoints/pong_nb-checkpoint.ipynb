{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-410d3c0873a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Playing Pong using the DQN algorithm\n",
    "\"\"\"\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    The policy network for appoximation of the Q function\n",
    "    Model Parameters like in Mnih et al., 2015\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions=4, feature_size=3136):\n",
    "        super(DQN, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.feature_size = feature_size\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(self.feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, self.n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"\n",
    "    Object for saving the memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, experience):\n",
    "        \"\"\"\n",
    "        Saves an experience or just one timepoint\n",
    "        :param experience:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.position >= self.capacity:\n",
    "            self.position = 0\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = experience\n",
    "        self.position += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Take a random batch from the memory\n",
    "        :param batch_size:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class Experience:\n",
    "    \"\"\"\n",
    "    Class with slots to save an experience\n",
    "    \"\"\"\n",
    "    __slots__ = ['state', 'action', 'reward', 'next_state', 'non_final']\n",
    "\n",
    "    def __init__(self, state, action, reward, next_state):\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_state = next_state\n",
    "        self.non_final = next_state is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, epsilon, q_network, n_actions):\n",
    "    \"\"\"\n",
    "    Choose an action given a state, epsilon and the q_network\n",
    "    :param state: \n",
    "    :param epsilon: \n",
    "    :param q_network: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor(random.randrange(n_actions))\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(q_network.forward(state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_frame(frame):\n",
    "    \"\"\"\n",
    "    Transform a frame to tensor of shape (batch_size, 1, 84, 84)\n",
    "    :param frame:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    frame = T.Compose([T.ToPILImage(), T.Resize((84, 84)), T.Grayscale(), T.ToTensor()])(frame)\n",
    "    frame = frame / 255\n",
    "    return frame\n",
    "\n",
    "def game_step(env, action, n_steps=4):\n",
    "    \"\"\"\n",
    "    Play one step of the game\n",
    "    :param env:\n",
    "    :param action:\n",
    "    :n_steps: number of frames to play\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    reward = 0\n",
    "    last_state = False\n",
    "    for i in range(n_steps):\n",
    "        frame, r, done, _ = env.step(action)\n",
    "        if done:\n",
    "            last_state = True\n",
    "            reward += r\n",
    "        else:\n",
    "            frames.append(transform_frame(frame))\n",
    "            reward += r\n",
    "    if last_state:\n",
    "        state = None\n",
    "    else:\n",
    "        state = torch.stack(frames, 0)\n",
    "        state = state.transpose(0, 1)\n",
    "\n",
    "    return (state, reward, last_state)\n",
    "\n",
    "def extract_batch(batch, slot):\n",
    "    if slot == 'action':\n",
    "        return torch.tensor([x.action for x in batch])\n",
    "    elif slot == 'state':\n",
    "        return torch.cat([x.state for x in batch])\n",
    "    elif slot == 'reward':\n",
    "        return torch.tensor([x.reward for x in batch])\n",
    "    elif slot == 'next_state':\n",
    "        # return only non-final next states\n",
    "        next_states = [x.next_state for x in batch]\n",
    "        non_final_next_states = torch.cat([s for s in next_states if s is not None])\n",
    "        return non_final_next_states\n",
    "    elif slot == 'non_final':\n",
    "        return torch.tensor([x.non_final for x in batch])\n",
    "    else:\n",
    "        raise Exception(\"Incorrect Experience slot specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(policy, target, memory, optimizer, criterion, batch_size=32, gamma=0.99, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Calculate loss for one batch and perform optimization\n",
    "    :param policy:\n",
    "    :param memory:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # check if enough memory has been aquired\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample batch from memory\n",
    "    batch = memory.sample(batch_size)\n",
    "\n",
    "    # extract state, action, reward, next_state\n",
    "    action_batch = extract_batch(batch, 'action').to(device).unsqueeze(1)\n",
    "    state_batch = extract_batch(batch, 'state').to(device)\n",
    "    reward_batch = extract_batch(batch, 'reward').to(device)\n",
    "    next_state_batch = extract_batch(batch, 'next_state').to(device)\n",
    "    non_final_mask = extract_batch(batch, 'non_final')\n",
    "    \n",
    "    # q-values\n",
    "    q_value = policy(state_batch)\n",
    "    q_value = q_value.gather(1, action_batch).squeeze(1)\n",
    "\n",
    "    # q-values for next state\n",
    "    target_q_value_all = torch.zeros(batch_size, device=device)\n",
    "    target_q_value = target(next_state_batch)\n",
    "    _, max_idx = torch.max(target_q_value, dim=1)\n",
    "    target_q_value = target_q_value.gather(1, max_idx.unsqueeze(1)).squeeze(1)\n",
    "    target_q_value_all[non_final_mask] = target_q_value\n",
    "    \n",
    "    # expected q-value\n",
    "    expected_q_value = reward_batch + (gamma * target_q_value_all)\n",
    "\n",
    "    loss = criterion(q_value, expected_q_value)\n",
    "\n",
    "    # Optimization step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -21.0, Epsilon: 0.9911230000000135, Memory: 269, Frames: 269\n",
      "Reward: -21.0, Epsilon: 0.980959000000029, Memory: 577, Frames: 577\n",
      "Reward: -21.0, Epsilon: 0.9718840000000428, Memory: 852, Frames: 852\n",
      "Reward: -20.0, Epsilon: 0.9605650000000601, Memory: 1195, Frames: 1195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-307-fca08755ec06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# make one step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mcomplete_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-300-7fc738dd94b2>\u001b[0m in \u001b[0;36mgame_step\u001b[0;34m(env, action, n_steps)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rl/lib/python3.7/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#=== PARAMETERS ===#\n",
    "batch_size = 32\n",
    "max_screens = 1400000\n",
    "memory_capacity = 80000\n",
    "memory_init_size = 500\n",
    "gamma = 0.99\n",
    "target_update = 10000\n",
    "epsilon_start = 1\n",
    "epsilon_end = 0.01\n",
    "epsilon_steps = 30000\n",
    "n_steps = 4\n",
    "lr = 0.0000625\n",
    "\n",
    "# Create Breakout environment\n",
    "env = gym.make('Pong-v0')\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# CNNs\n",
    "policy = DQN(n_actions=n_actions).cuda()\n",
    "target = DQN(n_actions=n_actions).to(device)\n",
    "target.load_state_dict(policy.state_dict())\n",
    "\n",
    "# Loss\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "# Memory\n",
    "memory = ReplayBuffer(memory_capacity)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "epsilon_delta = (epsilon_start - epsilon_end)/epsilon_steps\n",
    "\n",
    "screens = 0\n",
    "\n",
    "while screens < max_screens:\n",
    "    \n",
    "    # reset env for new episode\n",
    "    env.reset()\n",
    "    # get initial state\n",
    "    state, _, _ = game_step(env, env.action_space.sample(), n_steps=n_steps)\n",
    "    complete_reward = 0\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    for t in count():\n",
    "        screens += 1\n",
    "        \n",
    "        # adjust epsilon\n",
    "        epsilon = epsilon - epsilon_delta\n",
    "        if epsilon <= epsilon_end:\n",
    "            epsilon = epsilon_end\n",
    "        \n",
    "        # choose action based on current state\n",
    "        action = choose_action(state.cuda(), epsilon, policy, n_actions)\n",
    "        \n",
    "        # make one step\n",
    "        next_state, reward, done = game_step(env, action, n_steps=n_steps)\n",
    "        complete_reward += reward\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "    \n",
    "        # save the current experience\n",
    "        memory.push(Experience(state, action, reward, next_state))\n",
    "        \n",
    "        # update state variable\n",
    "        state = next_state\n",
    "        \n",
    "        if screens > memory_init_size:\n",
    "            # Perform one step of training on the policy network\n",
    "            training_step(policy, target, memory, optimizer, criterion=criterion, batch_size=batch_size, gamma=gamma)\n",
    "\n",
    "            # Update the target network after 10000 frames seen\n",
    "            if screens % target_update == 0:\n",
    "                target.load_state_dict(policy.state_dict())\n",
    "                print(\"update target\")\n",
    "\n",
    "        if done:\n",
    "            mem_len = len(memory.memory)\n",
    "            print(f\"Reward: {complete_reward}, Epsilon: {epsilon}, Memory: {mem_len}, Frames: {screens}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl-env] *",
   "language": "python",
   "name": "conda-env-rl-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
